<!DOCTYPE html>
<html>

<head>
  
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>基于深度学习的中英机器翻译 | wu-kan</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="基于深度学习的中英机器翻译" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="实验简介" />
<meta property="og:description" content="实验简介" />
<link rel="canonical" href="http://localhost:4000/_posts/2020-01-14-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%AD%E8%8B%B1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/" />
<meta property="og:url" content="http://localhost:4000/_posts/2020-01-14-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%AD%E8%8B%B1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/" />
<meta property="og:site_name" content="wu-kan" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-14T00:00:00+08:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/_posts/2020-01-14-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%AD%E8%8B%B1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/","headline":"基于深度学习的中英机器翻译","dateModified":"2020-01-14T00:00:00+08:00","datePublished":"2020-01-14T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/_posts/2020-01-14-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%AD%E8%8B%B1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"},"description":"实验简介","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  
  
  <meta
  name="viewport"
  content="width=device-width, initial-scale=1.0, maximum-scale=1"
/>
<meta
  http-equiv="content-type"
  content="text/html; charset=utf-8"
/>
<link
  rel="alternate"
  href="/feed.xml"
  title="RSS"
  type="application/rss+xml"
/>

  
  <link
  rel="apple-touch-icon-precomposed"
  href="https://gravatar.loli.net/avatar/289efba375d63424de3c49569c446744?s=320"
/>
<link
  rel="shortcut
  icon"
  href="https://gravatar.loli.net/avatar/289efba375d63424de3c49569c446744?s=32"
/>

  
  <link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/combine/gh/poole/lanyon@v1.1.0/public/css/poole.min.css,gh/poole/lanyon@v1.1.0/public/css/lanyon.min.css"
/>

  
  <link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"
/>

  
  <link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/Dreamer-Paul/Pio@2.4/static/pio.min.css"
/>
<script
  async="async"
  src="https://cdn.jsdelivr.net/combine/gh/Dreamer-Paul/Pio@2.4/static/l2d.min.js,gh/Dreamer-Paul/Pio@2.4/static/pio.min.js"
  onload='
      let pio_container = document.createElement("div");
      pio_container.classList.add("pio-container");
      pio_container.classList.add("right");
      pio_container.style.bottom = "-2rem";
      pio_container.style.zIndex = "1";
      document.body.insertAdjacentElement("beforeend", pio_container);
      let pio_action = document.createElement("div");
      pio_action.classList.add("pio-action");
      pio_container.insertAdjacentElement("beforeend", pio_action);
      let pio_canvas = document.createElement("canvas");
      pio_canvas.id = "pio";
      pio_canvas.style.width = "14rem";
      pio_canvas.width = "600";
      pio_canvas.height = "800";
      pio_container.insertAdjacentElement("beforeend", pio_canvas);
      let pio = new Paul_Pio({
        "mode": "fixed",
        "hidden": true,
        "night": "for(let i=7; i<16; ++i) if(document.body.classList.contains(`theme-base-0`+i.toString(16))) { document.body.classList.remove(`theme-base-0`+i.toString(16)); document.body.classList.add(`theme-base-0`+((i-6)%9+7).toString(16)); break; }",
        "content": {
          "link": ["https://jekyll-theme-WuK.wu-kan.cn"],
          "skin": ["要换成我的朋友吗？", "让她放个假吧~"],
          "hidden": true,
          "custom": [{
            "selector": "a",
            "type": "link",
          }, {
            "selector": ".sidebar-toggle",
            "text": "打开侧边栏叭~"
          }, {
            "selector": ".effect-info",
            "text": "哇，你发现了什么！"
          }, {
            "selector": "#sidebar-search-input",
            "text": "想搜索什么呢？很多干货哦！"
          }, {
            "selector": "#toc",
            "text": "这是目录~"
          }, {
            "selector": ".page-title",
            "text": "这是标题~"
          }, {
            "selector": ".v",
            "text": "评论没有审核，要对自己的发言负责哦~"
          }]
        },
        "model": [
          "https:\/\/cdn.jsdelivr.net/gh/imuncle/live2d/model/33/model.2018.bls-winter.json",
          "https:\/\/cdn.jsdelivr.net/gh/imuncle/live2d/model/platelet-2/model.json",
          "https:\/\/cdn.jsdelivr.net/gh/imuncle/live2d/model/xiaomai/xiaomai.model.json",
          "https:\/\/cdn.jsdelivr.net/gh/imuncle/live2d/model/mashiro/seifuku.model.json",
          "https:\/\/cdn.jsdelivr.net/gh/imuncle/live2d/model/Violet/14.json",
          "https:\/\/cdn.jsdelivr.net/gh/xiaoski/live2d_models_collection/Kobayaxi/Kobayaxi.model.json",
          "https:\/\/cdn.jsdelivr.net/gh/xiaoski/live2d_models_collection/mikoto/mikoto.model.json",
          "https:\/\/cdn.jsdelivr.net/gh/xiaoski/live2d_models_collection/uiharu/uiharu.model.json"]
      });'
></script>

  
  <script
  src='https://zz.bdstatic.com/linksubmit/push.js'
  async="async"
></script>

  
  <script
  async="async"
  src="https://www.googletagmanager.com/gtag/js?id=UA-163543967-1"
  onload="
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-163543967-1');"
></script>

  
  <style>
  .wrap {
    transition-property: all;
    transition-duration: .3s;
    transition-timing-function: ease-in-out;
    min-height: 100%;
    display: inline-block;
    background-size: 100% auto;
    background-position: 0% 0%;
    background-repeat: no-repeat;
    background-attachment: fixed;
    background-image: url(https://Mizuno-Ai.wu-kan.cn/pixiv/74559485_p1.webp);
  }
  @media (min-aspect-ratio: 2400/1850) {
    .wrap {
      background-image: url(https://Mizuno-Ai.wu-kan.cn/pixiv/71932901_p0.webp);
    }
  }
  .sidebar-overlay #sidebar-checkbox:checked ~ .wrap {
    width: calc(100% - 14rem);
    background-size: calc(100% - 14rem) auto;
    left: 14rem;
  }
  .layout-reverse.sidebar-overlay #sidebar-checkbox:checked ~ .wrap {
    left: 0;
  }
</style>

  
  <style>
  html,
  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  .sidebar {
    font-family: PingFang SC, Menlo, Monaco, "Courier New", Microsoft JhengHei, monospace;
  }
</style>

  
  <style>
  img {
    display: inline-block;
    margin: 0;
  }
</style>

  
  <style>
  ::-webkit-scrollbar {
    width: 4px;
    height: 4px;
  }
  ::-webkit-scrollbar-thumb {
    background-image: linear-gradient(45deg, Cyan 0%, Magenta 50%, Yellow 100%);
  }
</style>

  
  <style>
  ::selection {
    color: White;
    background: Black;
  }
</style>

  
</head>

<body
  class="theme-base-07 layout-reverse sidebar-overlay">
  
  
  
  <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
  <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"
     />
  <!-- Toggleable sidebar -->
  <div class="sidebar" id="sidebar">
    
    <div class="sidebar-item">
      <div class="effect effect-right_to_left">
        <img class="effect-img" src="https://gravatar.loli.net/avatar/289efba375d63424de3c49569c446744?s=320" alt="img" />
        <div class="effect-info">
          SYSU超算17级在读<br/>
永远喜欢水野爱<br/>
田宫例四驱车<br/>
ASC<br/>
<a href="mailto:i@wu-kan.cn">
  <i class="fas fa-envelope"></i>
</a>
<a href="https://github.com/wu-kan">
  <i class="fab fa-github"></i>
</a>
<a href="https://codeforces.com/profile/WuK">
  <i class="fas fa-chart-bar"></i>
</a>
<a href="https://vjudge.net/user/WuK">
  <i class="fas fa-smile"></i>
</a>
<a href="https://www.zhihu.com/people/wu.kan/activities">
  <i class="fab fa-zhihu"></i>
</a>
<iframe
  src="https://music.163.com/outchain/player?type=0&id=155059595&auto=0&height=32"
  width=100%
  height=52
  frameborder="no"
  border="0"
  marginwidth="0"
  marginheight="0"
></iframe>

        </div>
      </div>
    </div>
    
    <nav class="sidebar-nav">
      
      <a class="sidebar-nav-item" href="/">
        <i class="fas fa-home fa-fw"></i> 首页
      </a>
      
      <a class="sidebar-nav-item" href="/comments/">
        <i class="fas fa-comments fa-fw"></i> 留言
      </a>
      
      <a class="sidebar-nav-item" href="/tags/">
        <i class="fas fa-tags fa-fw"></i> 标签
      </a>
      
      <a class="sidebar-nav-item" href="/archive/">
        <i class="fas fa-archive fa-fw"></i> 归档
      </a>
      
      <a class="sidebar-nav-item" href="/merger/">
        <i class="fas fa-coffee fa-fw"></i> 打赏
      </a>
      
    </nav>
    <div class="sidebar-item">
      
      <div>
        <style>
  #sidebar-search-input {
    background: none;
    border: none;
    color: White;
    width: 100%;
  }
  #sidebar-search-results-container {
    overflow: auto auto;
    max-height: 50vh;
  }
</style>
<input
  id="sidebar-search-input"
  placeholder="搜索博文"
/>
<ol
  id="sidebar-search-results-container"
></ol>
<script
  src='https://cdn.jsdelivr.net/npm/simple-jekyll-search/dest/simple-jekyll-search.min.js'
  async='async'
  onload='
    SimpleJekyllSearch({
      json: "/assets/simple-jekyll-search/search.json",
      searchInput: document.getElementById("sidebar-search-input"),
      resultsContainer: document.getElementById("sidebar-search-results-container"),
      searchResultTemplate: `<li><a href="{url}">{title}</a></li>`,
      limit: 999,
      fuzzy: true
    })'
></script>

      </div>
      
      
      <style>
  .sidebar-checkbox {
    display: none;
  }
  .sidebar-toggle {
    position: fixed;
  }
</style>

      
      <style>
  .effect {
    margin: 1rem;
    perspective: 900px;
  }
  .effect-info {
    text-align: center;
    backface-visibility: hidden;
    position: absolute;
    top: 0;
    transform-style: preserve-3d;
  }
  .effect-img {
    z-index: 11;
    width: 100%;
    height: 100%;
    position: relative;
    transition: all 0.5s ease-in-out;
  }
  .effect-img:before {
    position: absolute;
    display: block;
  }
  .effect-right_to_left .effect-img {
    transform-origin: 0% 50%;
  }
  .effect-right_to_left:hover .effect-img {
    transform: rotate3d(0, 1, 0, -180deg);
  }
</style>

      
      <style>
  #toc {
    overflow: auto auto;
    max-height:50vh;
  }
</style>
<aside id="toc">
  目录
</aside>
<script
  defer='defer'
  src='https://cdn.jsdelivr.net/npm/html-contents/html-contents.min.js'
  onload="htmlContents('#toc', {listType: 'o', filter: function(arr) {return !arr.matches('.masthead-title')}})"
></script>

      
      <div>
  <i class="fas fa-cog fa-spin fa-fw"></i>
  <span id="run_time_day">
    <i class="fas fa-spinner fa-pulse"></i>
  </span>天
  <span id="run_time_hour">
    <i class="fas fa-spinner fa-pulse"></i>
  </span>时
  <span id="run_time_minute">
    <i class="fas fa-spinner fa-pulse"></i>
  </span>分
  <span id="run_time_second">
    <i class="fas fa-spinner fa-pulse"></i>
  </span>秒
  <script>
    setInterval(function (BirthDay) {
      function setzero(i) {
        if (i < 10) return "0" + i;
        return i;
      }
      BirthDay = new Date(BirthDay);
      today = new Date();
      timeold = (today.getTime() - BirthDay.getTime());
      sectimeold = timeold / 1000;
      secondsold = Math.floor(sectimeold);
      msPerDay = 24 * 60 * 60 * 1000;
      e_daysold = timeold / msPerDay;
      daysold = Math.floor(e_daysold);
      e_hrsold = (e_daysold - daysold) * 24;
      hrsold = Math.floor(e_hrsold);
      e_minsold = (e_hrsold - hrsold) * 60;
      minsold = Math.floor((e_hrsold - hrsold) * 60);
      seconds = Math.floor((e_minsold - minsold) * 60);
      document.getElementById("run_time_day").innerHTML = daysold;
      document.getElementById("run_time_hour").innerHTML = setzero(hrsold);
      document.getElementById("run_time_minute").innerHTML = setzero(minsold);
      document.getElementById("run_time_second").innerHTML = setzero(seconds);
    }, 1000, "10/04/2017 11:03:56") // 这是我第一篇CSDN博客的时间
  </script>
</div>

      
      <div>
  <div>
    <i class="fas fa-eye fa-fw"></i>
    <span id="busuanzi_value_page_pv">
      <i class="fas fa-spinner fa-pulse"></i>
    </span>次
  </div>
  <div>
    <i class="fas fa-paw fa-fw"></i>
    <span id="busuanzi_value_site_pv">
      <i class="fas fa-spinner fa-pulse"></i>
    </span>枚
  </div>
  <div>
    <i class="fas fa-user-friends fa-fw"></i>
    <span id="busuanzi_value_site_uv">
      <i class="fas fa-spinner fa-pulse"></i>
    </span>人
  </div>
  <script
    src='https://cdn.jsdelivr.net/npm/busuanzi'
    async='async'
  ></script>
</div>

      
      <div>
  <i class="fas fa-copyright fa-fw"></i>
  2017-2020 WuK
</div>

      
      <div>
  <i class="fas fa-thumbs-up fa-fw"></i>
  <a href="https://jekyll-theme-WuK.wu-kan.cn">
    jekyll-theme-WuK
  </a>
</div>

      
      <div>
  <i class="fas fa-info-circle fa-fw"></i>
  <a href="http://beian.miit.gov.cn">
    粤ICP备20024947号
  </a>
</div>

      
      
    </div>
  </div>
  <!-- Wrap is the content to shift when toggling the sidebar. We wrap the content to avoid any CSS collisions with our real content. -->
  
  <div class="wrap">
    
<style>
  pre {
    max-height: 50vh;
    overflow: auto;
  }
</style>


<style>
  @media (min-width: 56em) {
    .container {
      max-width: 66.6%;
    }
  }
</style>


<style>
  .masthead,
  .container.content {
    padding-top: 1rem;
    padding-bottom: 1rem;
    box-shadow: 0 0 .75rem rgba(0, 0, 0, 0.1);
    background-color: rgba(255, 255, 255, 0.95);
    animation-duration: 2s;
    animation-name: fadeIn;
  }
  @keyframes fadeIn {
    from {
      opacity: 0;
    }
    to {
      opacity: 1;
    }
  }
</style>


<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/combine/npm/prismjs/plugins/line-numbers/prism-line-numbers.min.css,npm/prismjs/plugins/toolbar/prism-toolbar.min.css,gh/PrismJS/prism-themes@1955cfef6953b3a59e66016e8a1e016b45d6cc79/themes/prism-nord.min.css"
/>
<script
  src="https://cdn.jsdelivr.net/combine/npm/prismjs/components/prism-core.min.js,npm/prismjs/plugins/autoloader/prism-autoloader.min.js,npm/prismjs/plugins/line-numbers/prism-line-numbers.min.js,npm/prismjs/plugins/toolbar/prism-toolbar.min.js"
  defer="defer"
  onload='
    Prism.plugins.autoloader.languages_path = "https:\/\/cdn.jsdelivr.net/npm/prismjs/components/";
    for(let x=document.getElementsByTagName("pre"), i=0;i<x.length;i++)
    {
      x[i].classList.add("line-numbers");
    }
    Prism.plugins.toolbar.registerButton("select-code", function (env) {
      let button = document.createElement("button");
      button.innerHTML = "select this " + env.language;
      button.addEventListener("click", function () {
        if (document.body.createTextRange) {
          let range = document.body.createTextRange();
          range.moveToElementText(env.element);
          range.select();
        } else if (window.getSelection) {
          let selection = window.getSelection();
          let range = document.createRange();
          range.selectNodeContents(env.element);
          selection.removeAllRanges();
            selection.addRange(range);
        }
      });
      return button;
    })'
></script>


<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"
/>
<script
  src="https://cdn.jsdelivr.net/combine/npm/katex/dist/katex.min.js,npm/katex/dist/contrib/mathtex-script-type.min.js,npm/katex/dist/contrib/auto-render.min.js"
  defer="defer"
  onload='renderMathInElement(document.body, { delimiters: [{ left: "$", right: "$", display: false }] })'
></script>


<style>
  pre.language-mermaid,
  code.language-mermaid {
    display: none;
  }
</style>
<script
  src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"
  defer="defer"
  onload='
  for(let x=document.getElementsByClassName("language-mermaid"), i=0;i<x.length;i++)
    if(x[i].nodeName=="CODE")
    {
      let m = document.createElement("div");
      m.classList.add("mermaid");
      m.textContent = x[i].textContent;
      x[i].parentNode.insertAdjacentElement("beforebegin", m);
    }'
></script>



<div class="masthead">
  <h3 class="container masthead-title">
    
    基于深度学习的中英机器翻译
    <a href="http://localhost:4000/" title="Home">
      <small>
        wu-kan
      </small>
    </a>
    
  </h3>
</div>

<div class="container content">
  <div class="post">
  <span class="post-date">
    
    <i class="fas fa-calendar-day fa-fw"></i>
    14 Jan 2020
    
    
    <i class="fas fa-file-word fa-fw"></i>
    16545字
    
    
    <i class="fas fa-clock fa-fw"></i>
    56分
    
    
    
    <i class="fas fa-tag fa-fw"></i>
    自然语言处理
    
    <br/>
<i class="fas fa-coffee fa-fw"></i>
<a href="/merger/">如果这篇博客帮助到你，可以请我喝一杯咖啡~</a>
<br/>
<i class="fab fa-creative-commons-by fa-fw"></i>
<a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">CC BY 4.0</a>（除特别声明或转载文章外）

    
  </span>
  <h2 id="实验简介">实验简介</h2>

<ul>
  <li>从 WMT18（News Commentary）中抽取的中英句子对
    <ul>
      <li>数据集规模：10000
        <ul>
          <li>10K 版本，train: 8000, test: 2000, dev: 2000</li>
          <li>100K 版本（非必须），train: 80000, test: 20000, dev: 20000</li>
          <li>可根据提供的数据处理脚本自定义数据集大小（不能太小）</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>数据格式
    <ul>
      <li>source: 每行一条中文句子</li>
      <li>target: 每行一条 source 中对应行数的英文句子</li>
    </ul>
  </li>
  <li>平台：Pytorch</li>
  <li>模型要求：
    <ul>
      <li>两个 LSTM 分别作为 Encoder 和 Decoder</li>
      <li>实现基于注意力机制的机器翻译</li>
      <li>自行选择分词工具</li>
      <li>改变 teacher forcing ratio，观察效果</li>
      <li>Beam Search 策略</li>
    </ul>
  </li>
  <li>评估指标：BLEU 值（BLEU-4）</li>
  <li>词向量：随机初始化或自选预训练词向量</li>
  <li>设备：CPU/GPU</li>
  <li>要求：
    <ul>
      <li>描述清楚核心代码逻辑和 tensor 维度</li>
      <li>描述清楚代码的运行环境和软件版本</li>
      <li>独立完成，不得抄袭！不得抄袭！不得抄袭！</li>
    </ul>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<h3 id="参考资料">参考资料</h3>

<ul>
  <li><a href="https://baike.baidu.com/item/Tensorflow%EF%BC%9A%E5%AE%9E%E6%88%98Google%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6">Tensorflow：实战 Google 深度学习框架</a>, 才云科技 Caicloud,郑泽宇,顾思宇</li>
  <li><a href="https://www.h3399.cn/201802/544465.html">浅谈用 Python 计算文本 BLEU 分数</a></li>
  <li><a href="https://tensorflow.google.cn/tutorials/text/nmt_with_attention">基于注意力的神经机器翻译</a></li>
  <li><a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION</a></li>
</ul>

<h3 id="相关论文">相关论文</h3>

<ul>
  <li>Effective Approaches to Attention-based Neural Machine Translation, Luong et al., EMNLP 2015</li>
  <li>Neural Machine Translation by Jointly Learning to Align and Translate, Bahdanau et al., ICLR 2015</li>
  <li>Bleu: a Method for Automatic Evaluation for Machine Translation, Papineni et al., ACL 2002</li>
</ul>

<h2 id="实验环境">实验环境</h2>

<h3 id="本地环境">本地环境</h3>

<p>所用机器型号为 VAIO Z Flip 2016。</p>

<ul>
  <li>Intel(R) Core(TM) i7-6567U CPU @3.30GHZ 3.31GHz</li>
  <li>8.00GB RAM</li>
  <li>Windows 10, 64-bit (Build 17763) 10.0.17763</li>
  <li>Visual Studio Code 1.39.2
    <ul>
      <li>Python 2019.10.41019：九月底发布的 VSCode Python 插件支持在编辑器窗口内原生运行 juyter nootbook 了，非常赞！</li>
      <li>Remote - WSL 0.39.9：配合 WSL，在 Windows 上获得 Linux 接近原生环境的体验。</li>
    </ul>
  </li>
  <li>Windows Subsystem for Linux [Ubuntu 18.04.2 LTS]：WSL 是以软件的形式运行在 Windows 下的 Linux 子系统，是近些年微软推出来的新工具，可以在 Windows 系统上原生运行 Linux。
    <ul>
      <li>Python 3.7.4 64-bit (‘anaconda3’:virtualenv)：安装在 WSL 中。
        <ul>
          <li>jieba==0.39</li>
          <li>nltk==3.3</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="集群环境">集群环境</h3>

<p>借用 V100 集群中的一个节点，硬件参数和软件配置如下。属于相当豪华的配置了，和上次实验相比这次不到一个小时就跑完了，有钱真好…</p>

<pre><code class="language-bash">$ NodeName=gpu27 Arch=x86_64 CoresPerSocket=14
   CPUAlloc=28 CPUTot=28 CPULoad=4.05
   AvailableFeatures=(null)
   ActiveFeatures=(null)
   Gres=(null)
   NodeAddr=gpu27 NodeHostName=gpu27
   OS=Linux 3.10.0-957.el7.x86_64 #1 SMP Sat Oct 12 02:01:43 CST 2019
   RealMemory=256000 AllocMem=0 FreeMem=219734 Sockets=2 Boards=1
   State=ALLOCATED ThreadsPerCore=1 TmpDisk=0 Weight=0 Owner=N/A MCS_label=N/A
   Partitions=gpu_v100
   BootTime=2020-01-10T15:25:50 SlurmdStartTime=2020-01-10T15:32:49
   CfgTRES=cpu=28,mem=250G,billing=28
   AllocTRES=cpu=28,mem=250G,billing=28
   CapWatts=n/a
   CurrentWatts=0 AveWatts=0
   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s
$ module list
Currently Loaded Modulefiles:
 1) CUDA/10.0              3) TensorFlow/1.13.2-gpu-py3.6-cuda10
 2) cudnn/7.4.1-CUDA10.0
$ nvidia-smi
Tue Jan 14 08:25:27 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:8A:00.0 Off |                    0 |
| N/A   30C    P0    37W / 300W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:8B:00.0 Off |                    0 |
| N/A   27C    P0    37W / 300W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:B3:00.0 Off |                    0 |
| N/A   28C    P0    37W / 300W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:B4:00.0 Off |                    0 |
| N/A   29C    P0    37W / 300W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>

<h2 id="实验过程">实验过程</h2>

<p>由于集群并没有连外网，数据清洗、分词时需要安装一些依赖和包的，因此都在本地完成了，而核心的深度学习过程是在集群上进行。</p>

<h3 id="预处理preprocesspy">预处理<code>preprocess.py</code></h3>

<p>使用<code>jieba</code>对中文文本进行分词，并做了数据清洗工作：使用英文标点替代中文标点；并洗掉了原来数据中的一些乱码。</p>

<pre><code class="language-python">def getCMN(cmn):
    import jieba
    cmn_lines = open(cmn, 'r', encoding='utf-8').readlines()
    for i in range(len(cmn_lines)):
        s = ' '.join(jieba.cut(cmn_lines[i], cut_all=False))
        s = re.sub(r"&amp;#[0-9]+;", r"", s)
        s = re.sub(r"�", r"", s)
        for zh, en in [("。", "."), ("！", "!"), ("？", "?"), ("，", ",")]:
            s = s.replace(zh, en)
        s = re.sub(u"[^a-zA-Z0-9\u4e00-\u9fa5,.!?]", u" ", s)
        s = re.sub(r"\s+", r" ", s)
        cmn_lines[i] = s.lower().strip().split()
    return cmn_lines
</code></pre>

<p>英文部分的处理更加简单，这里不直接放出了。</p>

<p>下面是根据上一步分词的结果建立词典的过程。由于在数据集中数据显然不会是均匀分布的，常用词汇的频率明显会高于冷门词汇，因此我按照出现的频数从大到小排序并删去了频数最小的那部分词。这样做的道理是，冷门词汇很少会出现在翻译结果中，如果保留下来的话会增大词向量但并不能给最终的结果带来多少好的改进，反而会大大减慢我们学习的效率。</p>

<p>此外，我添加了三个标记词：</p>

<ul>
  <li><code>&lt;unk&gt;</code>：代表未出现或难匹配，索引 0</li>
  <li><code>&lt;sos&gt;</code>：代表句子开头，索引 1</li>
  <li><code>&lt;eos&gt;</code>：代表句子结尾，索引 2</li>
</ul>

<pre><code class="language-python">def build_vocab(lang, vocab, vocab_size):
    counter = {}
    for line in lang:
        for word in line:
            if word not in counter:
                counter[word] = 0
            counter[word] += 1
    id_to_word = ["&lt;unk&gt;", "&lt;sos&gt;", "&lt;eos&gt;"]+[x[0]
                                              for x in sorted(counter.items(), key=lambda x: x[1], reverse=True)]
    if len(id_to_word) &gt; vocab_size:
        id_to_word = id_to_word[:vocab_size]
    with open(vocab, 'w', 'utf-8') as f:
        for word in id_to_word:
            f.write(word + '\n')
</code></pre>

<p>最后是根据词典重新给数据集标号，同时为原来的数据集加上句尾标记<code>&lt;eos&gt;</code>。</p>

<pre><code class="language-python">def build_data(lang, vocab, data):
    id_to_word = open(vocab, 'r', encoding='utf-8').readlines()
    word_to_id = {}
    for i in range(len(id_to_word)):
        word_to_id[id_to_word[i].strip()] = i
    with open(data, 'w', 'utf-8') as fd:
        for line in lang:
            fd.write(' '.join(
                [str(word_to_id[w if w in word_to_id else '&lt;unk&gt;']) for w in line+['&lt;eos&gt;']]) + '\n')
</code></pre>

<h3 id="训练模型trainpy">训练模型<code>train.py</code></h3>

<p>此处我参考了《Tensorflow：实战 Google 深度学习框架》这本书中的教学代码，它的好处是内容深入浅出，并有比较细致的代码注释。</p>

<pre><code class="language-bash"># 定义NMTModel类来描述模型。
class NMTModel(object):
    # 在模型的初始化函数中定义模型要用到的变量。
    def __init__(self):
        # 定义编码器和解码器所使用的LSTM结构。
        self.enc_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)
        self.enc_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)
        self.dec_cell = tf.nn.rnn_cell.MultiRNNCell(
          [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)
           for _ in range(DECODER_LAYERS)])

        # 为源语言和目标语言分别定义词向量。
        self.src_embedding = tf.get_variable(
            "src_emb", [SRC_VOCAB_SIZE, HIDDEN_SIZE])
        self.trg_embedding = tf.get_variable(
            "trg_emb", [TRG_VOCAB_SIZE, HIDDEN_SIZE])

        # 定义softmax层的变量
        if SHARE_EMB_AND_SOFTMAX:
           self.softmax_weight = tf.transpose(self.trg_embedding)
        else:
           self.softmax_weight = tf.get_variable(
               "weight", [HIDDEN_SIZE, TRG_VOCAB_SIZE])
        self.softmax_bias = tf.get_variable(
            "softmax_bias", [TRG_VOCAB_SIZE])

    # 在forward函数中定义模型的前向计算图。
    # src_input, src_size, trg_input, trg_label, trg_size分别是上面
    # MakeSrcTrgDataset函数产生的五种张量。
    def forward(self, src_input, src_size, trg_input, trg_label, trg_size):
        batch_size = tf.shape(src_input)[0]

        # 将输入和输出单词编号转为词向量。
        src_emb = tf.nn.embedding_lookup(self.src_embedding, src_input)
        trg_emb = tf.nn.embedding_lookup(self.trg_embedding, trg_input)

        # 在词向量上进行dropout。
        src_emb = tf.nn.dropout(src_emb, KEEP_PROB)
        trg_emb = tf.nn.dropout(trg_emb, KEEP_PROB)

        # 使用dynamic_rnn构造编码器。
        # 编码器读取源句子每个位置的词向量，输出最后一步的隐藏状态enc_state。
        # 因为编码器是一个双层LSTM，因此enc_state是一个包含两个LSTMStateTuple类
        # 张量的tuple，每个LSTMStateTuple对应编码器中的一层。
        # 张量的维度是 [batch_size, HIDDEN_SIZE]。
        # enc_outputs是顶层LSTM在每一步的输出，它的维度是[batch_size,
        # max_time, HIDDEN_SIZE]。Seq2Seq模型中不需要用到enc_outputs，而
        # 后面介绍的attention模型会用到它。
        # 下面的代码取代了Seq2Seq样例代码中forward函数里的相应部分。
        with tf.variable_scope("encoder"):
            # 构造编码器时，使用bidirectional_dynamic_rnn构造双向循环网络。
            # 双向循环网络的顶层输出enc_outputs是一个包含两个张量的tuple，每个张量的
            # 维度都是[batch_size, max_time, HIDDEN_SIZE]，代表两个LSTM在每一步的输出。
            enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(
                self.enc_cell_fw, self.enc_cell_bw, src_emb, src_size,
                dtype=tf.float32)
            # 将两个LSTM的输出拼接为一个张量。
            enc_outputs = tf.concat([enc_outputs[0], enc_outputs[1]], -1)

        with tf.variable_scope("decoder"):
            # 选择注意力权重的计算模型。BahdanauAttention是使用一个隐藏层的前馈神经网络。
            # memory_sequence_length是一个维度为[batch_size]的张量，代表batch
            # 中每个句子的长度，Attention需要根据这个信息把填充位置的注意力权重设置为0。
            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(
                HIDDEN_SIZE, enc_outputs,
                memory_sequence_length=src_size)

            # 将解码器的循环神经网络self.dec_cell和注意力一起封装成更高层的循环神经网络。
            attention_cell = tf.contrib.seq2seq.AttentionWrapper(
                self.dec_cell, attention_mechanism,
                attention_layer_size=HIDDEN_SIZE)

            # 使用attention_cell和dynamic_rnn构造编码器。
            # 这里没有指定init_state，也就是没有使用编码器的输出来初始化输入，而完全依赖
            # 注意力作为信息来源。
            dec_outputs, _ = tf.nn.dynamic_rnn(
                attention_cell, trg_emb, trg_size, dtype=tf.float32)

        # 计算解码器每一步的log perplexity。这一步与语言模型代码相同。
        output = tf.reshape(dec_outputs, [-1, HIDDEN_SIZE])
        logits = tf.matmul(output, self.softmax_weight) + self.softmax_bias
        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
            labels=tf.reshape(trg_label, [-1]), logits=logits)

        # 在计算平均损失时，需要将填充位置的权重设置为0，以避免无效位置的预测干扰
        # 模型的训练。
        label_weights = tf.sequence_mask(
            trg_size, maxlen=tf.shape(trg_label)[1], dtype=tf.float32)
        label_weights = tf.reshape(label_weights, [-1])
        cost = tf.reduce_sum(loss * label_weights)
        cost_per_token = cost / tf.reduce_sum(label_weights)

        # 定义反向传播操作。反向操作的实现与语言模型代码相同。
        trainable_variables = tf.trainable_variables()

        # 控制梯度大小，定义优化方法和训练步骤。
        grads = tf.gradients(cost / tf.to_float(batch_size),
                             trainable_variables)
        grads, _ = tf.clip_by_global_norm(grads, MAX_GRAD_NORM)
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)
        train_op = optimizer.apply_gradients(
            zip(grads, trainable_variables))
        return cost_per_token, train_op
</code></pre>

<p>这里词向量我是使用随机初始化的。</p>

<pre><code class="language-python">initializer = tf.random_uniform_initializer(-0.05, 0.05)
</code></pre>

<h3 id="模型预测testpy">模型预测<code>test.py</code></h3>

<p>可以加载上一步中训练的模型对<strong>单个句子</strong>进行预测。注意这里我没有读取字典，而让预测的结果仍然是在词典中的索引。主要是因为集群上的环境还是很不稳定，使用<code>utf-8</code>编码的时候老是出现这样那样的报错；而在集群上调试还是不如本地折腾来得痛快。</p>

<pre><code class="language-python"># -*- coding: utf-8 -*-
# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'
# %%
import tensorflow as tf
import codecs
import sys
from config import *

# 读取checkpoint的路径。7600表示是训练程序在第7600步保存的checkpoint。
CHECKPOINT_PATH += "-7600"

# %% [markdown]
# #### 2.定义NMT模型和解码步骤。

# %%
# 定义NMTModel类来描述模型。


class NMTModel(object):
    # 在模型的初始化函数中定义模型要用到的变量。
    def __init__(self):
        # 定义编码器和解码器所使用的LSTM结构。
        self.enc_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)
        self.enc_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)
        self.dec_cell = tf.nn.rnn_cell.MultiRNNCell(
            [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)
             for _ in range(DECODER_LAYERS)])

        # 为源语言和目标语言分别定义词向量。
        self.src_embedding = tf.get_variable(
            "src_emb", [SRC_VOCAB_SIZE, HIDDEN_SIZE])
        self.trg_embedding = tf.get_variable(
            "trg_emb", [TRG_VOCAB_SIZE, HIDDEN_SIZE])

        # 定义softmax层的变量
        if SHARE_EMB_AND_SOFTMAX:
            self.softmax_weight = tf.transpose(self.trg_embedding)
        else:
            self.softmax_weight = tf.get_variable(
                "weight", [HIDDEN_SIZE, TRG_VOCAB_SIZE])
        self.softmax_bias = tf.get_variable(
            "softmax_bias", [TRG_VOCAB_SIZE])

    def inference(self, src_input):
        # 虽然输入只有一个句子，但因为dynamic_rnn要求输入是batch的形式，因此这里
        # 将输入句子整理为大小为1的batch。
        src_size = tf.convert_to_tensor([len(src_input)], dtype=tf.int32)
        src_input = tf.convert_to_tensor([src_input], dtype=tf.int32)
        src_emb = tf.nn.embedding_lookup(self.src_embedding, src_input)

        with tf.variable_scope("encoder"):
            # 使用bidirectional_dynamic_rnn构造编码器。这一步与训练时相同。
            enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(
                self.enc_cell_fw, self.enc_cell_bw, src_emb, src_size,
                dtype=tf.float32)
            # 将两个LSTM的输出拼接为一个张量。
            enc_outputs = tf.concat([enc_outputs[0], enc_outputs[1]], -1)

        with tf.variable_scope("decoder"):
            # 定义解码器使用的注意力机制。
            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(
                HIDDEN_SIZE, enc_outputs,
                memory_sequence_length=src_size)

            # 将解码器的循环神经网络self.dec_cell和注意力一起封装成更高层的循环神经网络。
            attention_cell = tf.contrib.seq2seq.AttentionWrapper(
                self.dec_cell, attention_mechanism,
                attention_layer_size=HIDDEN_SIZE)

        # 设置解码的最大步数。这是为了避免在极端情况出现无限循环的问题。
        MAX_DEC_LEN = 100

        with tf.variable_scope("decoder/rnn/attention_wrapper"):
            # 使用一个变长的TensorArray来存储生成的句子。
            init_array = tf.TensorArray(dtype=tf.int32, size=0,
                                        dynamic_size=True, clear_after_read=False)
            # 填入第一个单词&lt;sos&gt;作为解码器的输入。
            init_array = init_array.write(0, SOS_ID)
            # 调用attention_cell.zero_state构建初始的循环状态。循环状态包含
            # 循环神经网络的隐藏状态，保存生成句子的TensorArray，以及记录解码
            # 步数的一个整数step。
            init_loop_var = (
                attention_cell.zero_state(batch_size=1, dtype=tf.float32),
                init_array, 0)

            # tf.while_loop的循环条件：
            # 循环直到解码器输出&lt;eos&gt;，或者达到最大步数为止。
            def continue_loop_condition(state, trg_ids, step):
                return tf.reduce_all(tf.logical_and(
                    tf.not_equal(trg_ids.read(step), EOS_ID),
                    tf.less(step, MAX_DEC_LEN-1)))

            def loop_body(state, trg_ids, step):
                # 读取最后一步输出的单词，并读取其词向量。
                trg_input = [trg_ids.read(step)]
                trg_emb = tf.nn.embedding_lookup(self.trg_embedding,
                                                 trg_input)
                # 调用attention_cell向前计算一步。
                dec_outputs, next_state = attention_cell.call(
                    state=state, inputs=trg_emb)
                # 计算每个可能的输出单词对应的logit，并选取logit值最大的单词作为
                # 这一步的而输出。
                output = tf.reshape(dec_outputs, [-1, HIDDEN_SIZE])
                logits = (tf.matmul(output, self.softmax_weight)
                          + self.softmax_bias)
                next_id = tf.argmax(logits, axis=1, output_type=tf.int32)
                # 将这一步输出的单词写入循环状态的trg_ids中。
                trg_ids = trg_ids.write(step+1, next_id[0])
                return next_state, trg_ids, step+1

            # 执行tf.while_loop，返回最终状态。
            state, trg_ids, step = tf.while_loop(
                continue_loop_condition, loop_body, init_loop_var)
            return trg_ids.stack()

# %% [markdown]
# #### 翻译

# %%


def main():
    with codecs.open(OUT_TEST_DATA, 'a', 'utf-8') as f:
        input_id = codecs.open(
            SRC_TEST_DATA, 'r', 'utf-8').readlines()[int(sys.argv[1])]
        input_ids = [int(s) for s in input_id.strip().split()]
        # 定义训练用的循环神经网络模型。
        with tf.variable_scope("nmt_model", reuse=None):
            model = NMTModel()

        # 建立解码所需的计算图。

        output_op = model.inference(input_ids)

        sess = tf.Session()
        saver = tf.train.Saver()
        saver.restore(sess, CHECKPOINT_PATH)

        # 读取翻译结果。
        output_ids = sess.run(output_op)
        sess.close()
        for o in output_ids:
            f.write(str(o)+' ')
        f.write('\n')


if __name__ == "__main__":
    main()
</code></pre>

<h3 id="训练参数configpy">训练参数<code>config.py</code></h3>

<p>训练模型的超参数如下，对应的数值和注释已经在这个文件里了，只要<code>from config import *</code>即可包含到每个文件中。</p>

<pre><code class="language-python">SRC_TRAIN_DATA = "./train.zh"          # 源语言输入文件。
SRC_TEST_DATA = "./test.zh"          # 源语言输入文件。
TRG_TRAIN_DATA = "./train.en"          # 目标语言输入文件。
TRG_TEST_DATA = "./test.en"          # 源语言输入文件。
OUT_TEST_DATA = "./out.en"
CHECKPOINT_PATH = "./attention_ckpt"   # checkpoint保存路径。
# 词汇表文件
SRC_VOCAB = "./zh.vocab"
TRG_VOCAB = "./en.vocab"

# 词汇表中&lt;sos&gt;和&lt;eos&gt;的ID。在解码过程中需要用&lt;sos&gt;作为第一步的输入，并将检查
# 是否是&lt;eos&gt;，因此需要知道这两个符号的ID。
UNK_ID = 0
SOS_ID = 1
EOS_ID = 2

HIDDEN_SIZE = 1024                     # LSTM的隐藏层规模。
DECODER_LAYERS = 2                     # 解码器中LSTM结构的层数。这个例子中编码器固定使用单层的双向LSTM。
SRC_VOCAB_SIZE = 10000                 # 源语言词汇表大小。
TRG_VOCAB_SIZE = 4000                 # 目标语言词汇表大小。
BATCH_SIZE = 100                       # 训练数据batch的大小。
NUM_EPOCH = 100                        # 使用训练数据的轮数。
KEEP_PROB = 0.8                        # 节点不被dropout的概率。
MAX_GRAD_NORM = 5                      # 用于控制梯度膨胀的梯度大小上限。
SHARE_EMB_AND_SOFTMAX = True           # 在Softmax层和词向量层之间共享参数。

MAX_LEN = 50   # 限定句子的最大单词数量。
</code></pre>

<h3 id="评估效果gradepy">评估效果<code>grade.py</code></h3>

<p>最后是本地上对翻译效果的一个评估，这里直接使用了经典自然语言处理库<code>nltk</code>中的<code>sentence_bleu</code>进行实现。</p>

<pre><code class="language-python">from nltk.translate.bleu_score import sentence_bleu
from codecs import open
from config import *


if __name__ == '__main__':
    id_to_src = [x.strip() for x in open(SRC_VOCAB, 'r', 'utf-8').readlines()]
    id_to_trg = [x.strip() for x in open(TRG_VOCAB, 'r', 'utf-8').readlines()]

    questions = open(SRC_TEST_DATA, 'r', 'utf-8').readlines()
    reference = open(TRG_TEST_DATA, 'r', 'utf-8').readlines()
    candidate = open(OUT_TEST_DATA, 'r', 'utf-8').readlines()

    for i in range(len(questions)):
        print('&lt;', end='\t')
        for j in questions[i].split():
            print(id_to_src[int(j)], end=' ')
        print('\n=', end='\t')
        for j in reference[i].split():
            print(id_to_trg[int(j)], end=' ')
        print('\n&gt;', end='\t')
        for j in candidate[i].split():
            print(id_to_trg[int(j)], end=' ')

        print("\nBleu score = " +
              str(sentence_bleu([reference[i]], candidate[i]))+"\n")
</code></pre>

<h2 id="结果分析">结果分析</h2>

<h3 id="损失函数">损失函数</h3>

<p>训练过程中的损失函数变化如下。</p>

<p><img src="data:image/webp;base64,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" alt="Loss" /></p>

<p>可以看到训练前期训练是比较正常的，Loss 很快收敛，但是训练后期 Loss 不再下降，出现了过拟合的趋势。</p>

<h3 id="翻译结果">翻译结果</h3>

<p>选出几条比较有代表性的进行分析。其中<code>&lt;</code>开头的为源语言句子，<code>=</code>开头为目标语言句子，<code>&gt;</code>开头为模型预测结果。</p>

<p>这是预测结果里得分最低的一条，但是这里革命<code>revolution</code>和<code>a fresh start</code>其实意思上有那么点接近。主要原因是，评测集中每条源语言句子只提供了一条正确的目标语言句子，实际上一个句子可以有多种翻译方式，这就导致评测出来的 Bleu 值有很大局限性。</p>

<pre><code class="language-bash">&lt;	欧洲 的 容克 革命 &lt;eos&gt;
=	europe s &lt;unk&gt; revolution &lt;eos&gt;
&gt;	&lt;sos&gt; a fresh start for europe &lt;eos&gt;
Bleu score = 3.641778071011159e-78
</code></pre>

<p>可以看到下面的的结果和前一个结果非常接近，在机器看来两个句子非常相似，前一个翻译很低分，在这里的评分就比较高了。</p>

<pre><code class="language-bash">&lt;	有关 欧洲 增长 的 议程 &lt;eos&gt;
=	an agenda for growth in europe &lt;eos&gt;
&gt;	&lt;sos&gt; a german start for europe &lt;eos&gt;
Bleu score = 0.3384653583738009
</code></pre>

<p>而下面这一条是预测结果中得分最高的一条。由于我在数据清洗的时候将很多出现频度低的词直接标成<code>&lt;unk&gt;</code>，而在 Bleu 评分的过程中两个<code>&lt;unk&gt;</code>匹配也会被计分，因此它的评分有些虚高，参考价值不大。</p>

<pre><code class="language-bash">&lt;	就 像 许多 世纪 以前 &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; 被 人 在 &lt;unk&gt; 教堂 里 杀害 一样 , 是因为 &lt;unk&gt; 相信 这 将 会 &lt;unk&gt; 国王 . &lt;eos&gt;
=	like the murder of &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; in his &lt;unk&gt; &lt;unk&gt; many centuries ago , the crime was committed in the clear belief that it would &lt;unk&gt; the &lt;unk&gt; . &lt;eos&gt;
&gt;	&lt;sos&gt; like the &lt;unk&gt; , even the &lt;unk&gt; treaty believes that so many &lt;unk&gt; in the &lt;unk&gt; they have , the state would want to negotiate the &lt;unk&gt; . &lt;eos&gt;
Bleu score = 0.47808191993705373
</code></pre>

<p>下面一句也取得了较高的 Bleu 评分，可以看到其实机器翻译模型将前一句的时间信息已经提取的比较准确了。这很大原因是国际政治新闻中的时间描述比较多，具体到具体的事件之后辨识度就没有那么高了，因此后面开始乱翻译了。</p>

<pre><code class="language-bash">&lt;	自从 2001 年 9 月 11 日 以后 , 美国 就 一直 在 针对 穆斯林 社会 的 &lt;unk&gt; 进行 一场 &lt;unk&gt; 运动 . &lt;eos&gt;
=	ever since september 11 , 2001 , the united states has been &lt;unk&gt; in a &lt;unk&gt; against the forces of evil in the muslim world . &lt;eos&gt;
&gt;	&lt;sos&gt; since the events that has been overcome since september , the world has weakened consequences been entirely at the &lt;unk&gt; and weakened events in order . &lt;eos&gt;
Bleu score = 0.39792915797480105
</code></pre>

<p>这一条是我翻译结果里面我觉得最有意思的，结合最近的国际局势真的非常有灵性…</p>

<pre><code class="language-bash">&lt;	伊朗 是 矛盾 之地 . &lt;eos&gt;
=	iran is a land of &lt;unk&gt; . &lt;eos&gt;
&gt;	&lt;sos&gt; iran won t stop . &lt;eos&gt;
Bleu score = 0.31068218135768266
</code></pre>

<p>对测试集的结果做 Bleu 评分后的分布大致如下。</p>

<p><img src="data:image/webp;base64,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" alt="Bleu" /></p>

<h3 id="beam-search-策略">Beam Search 策略</h3>

<p>经过测试和比较，集束搜索策略对我的结果几乎没有改进，大部分搜索的结果都和之前的接近，这意味着大部分情况下机器翻译的模型都能找到概率值最大的路径。同时，集束搜索的时空成本都大大增加了，有点得不偿失。</p>

<h2 id="总结与思考遇到的困难及采用的解决方法后续改进方向等">总结与思考（遇到的困难及采用的解决方法、后续改进方向等）</h2>

<p>本学期的自然然语言处理课程终于结束，通过三次实验我体验了自然语言处理从数据挖掘、数据清洗再到模型搭建学习的完整过程。和本学期一些其他与人工智能相关的课程相比，这门课的机器学习的训练集合和训练时间都是最长的。好在使用用集群进行计算之后训练的速度确实大大加快，也能够塞更多的训练数据。听说大公司训练出的成熟对话模型的基本上都用了上 GB 级别的语料集合进行训练，那么看来做自然语言处理确实很吃计算资源。此外，在搭建模型的时候，要让写出来的模型能够运行，从数据爬取和清洗的过程就要很下一番功夫。然后，经过漫长时间的忙碌搭的模型终于跑出结果了，结果全是<code>and</code>、<code>the</code>一类的词，让人根本分不清到底是自己在实验的哪个过程出了问题，还是单纯的模型没有收敛完全，非常玄学和让人抓狂。调包调参上手很快，但是真正学好这门课，也完全不是一件容易的事。</p>

<p>记得自然语言处理的第一堂课上，老师说的这样一句话让人印象深刻：<strong>自然语言理解是人工智能皇冠上的明珠</strong>。和人工智能的另一大热门领域，计算机视觉比起来，自然语言处理这门学科的发展还处在发展更早期的阶段。虽然深度学习的热潮给自然语言处理领域带来了成果，但从理论方法的角度看，由于采集、整理、表示和有效应用大量知识的困难，这些系统更依赖于统计学的方法和其他“简单”的方法或技巧，而这些统计学的方法和其他“简单”的方法似乎也快达到它们的极限了。我非常期待这门学科在接下来的一段时期能够取得新理论的突破和机遇，为我们的生活提供更多的方便。</p>

</div>
<div class="v">
  <i class="fas fa-spinner fa-pulse"></i>
</div>
<script
  src='https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js'
  defer='defer'
  onload='
    new Valine({
      "el": document.getElementsByClassName("v")[0],
      "appId": "9hABRddSuEkTgqLrt1VSK5B1-gzGzoHsz",
      "appKey": "NJ7RwmgrxsF7KDzlqU7YewlL",
      "placeholder": "在这里评论吧！填写邮箱可以获得 Gravatar 头像和回复通知哦",
      "requiredFields": ["nick","mail"],
      "visitor": true,
      "recordIP": true
    })'
></script>

</div>
  </div>
  
  <label for="sidebar-checkbox" class="sidebar-toggle"></label>
  
</body>

</html>